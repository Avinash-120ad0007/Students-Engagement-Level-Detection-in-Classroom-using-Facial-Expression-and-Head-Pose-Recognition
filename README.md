This project aims to analyze the engagement levels of students during classroom lectures by processing video footage. By utilizing advanced computer vision techniques, we determine the concentration levels of students based on their facial emotions and head poses.

ğğ«ğ¨ğœğğ¬ğ¬ ğğ¯ğğ«ğ¯ğ¢ğğ°:

ğ‘½ğ’Šğ’…ğ’†ğ’ ğ‘­ğ’“ğ’‚ğ’ğ’† ğ‘¬ğ’™ğ’•ğ’“ğ’‚ğ’„ğ’•ğ’Šğ’ğ’:

The classroom lecture video is divided into individual frames by using OpenCV Techniques.

ğ‘­ğ’‚ğ’„ğ’† ğ‘«ğ’†ğ’•ğ’†ğ’„ğ’•ğ’Šğ’ğ’ ğ‘¼ğ’”ğ’Šğ’ğ’ˆ ğ‘´ğ‘»ğ‘ªğ‘µğ‘µ:

Each frame is processed using the MTCNN library to detect faces.
The detected faces are then cropped using the coordinates provided by MTCNN.

ğ‘­ğ’‚ğ’„ğ’Šğ’‚ğ’ ğ‘¬ğ’ğ’ğ’•ğ’Šğ’ğ’ ğ‘¹ğ’†ğ’„ğ’ğ’ˆğ’ğ’Šğ’•ğ’Šğ’ğ’:

The cropped face images are passed through a Facial Emotion Recognition model based on the MaNet architecture.
This model classifies the facial emotion of each student in the frame.

ğ‘¯ğ’†ğ’‚ğ’… ğ‘·ğ’ğ’”ğ’† ğ‘¬ğ’”ğ’•ğ’Šğ’ğ’‚ğ’•ğ’Šğ’ğ’:

The cropped images are also passed through a Head Pose Estimation model.
Using the AFLW 2000 dataset, a Support Vector Regression (SVR) model was trained on the x, y, z coordinates of key facial landmarks (e.g., eyes, nose, lips) to determine the direction the student is looking.

ğ‘ªğ’ğ’ğ’„ğ’†ğ’ğ’•ğ’“ğ’‚ğ’•ğ’Šğ’ğ’ ğ‘°ğ’ğ’…ğ’†ğ’™ ğ‘ªğ’‚ğ’ğ’„ğ’–ğ’ğ’‚ğ’•ğ’Šğ’ğ’:

Each student's emotion and head pose are assigned a weight to calculate their concentration index.
The average concentration index of the entire class is computed for each frame and can be aggregated over the whole lecture.
Datasets Used

ğ‘­ğ’‚ğ’„ğ’Šğ’‚ğ’ ğ‘¬ğ’ğ’ğ’•ğ’Šğ’ğ’ ğ‘¹ğ’†ğ’„ğ’ğ’ˆğ’ğ’Šğ’•ğ’Šğ’ğ’:

Model is trained using the RAFDB Dataset Dataset Link.
Model architecture based on the MaNet research paper Paper Link.

ğ‘¯ğ’†ğ’‚ğ’… ğ‘·ğ’ğ’”ğ’† ğ‘¬ğ’”ğ’•ğ’Šğ’ğ’‚ğ’•ğ’Šğ’ğ’:

Model is trained using the AFLW 2000 Dataset Dataset Link.
Key facial landmarks were used to capture x, y, z coordinates for training the SVR model.
